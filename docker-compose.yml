version: "3"

services:
  zookeeper:
    container_name: "zookeeper"
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
      
  kafka:
    container_name: "kafka"
    image: 'wurstmeister/kafka:latest'
    ports:
      - '9092:9092'
      - '9094:9094'
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # KAFKA_ADVERTISED_HOST_NAME: instance-1.southamerica-east1-a.c.sincere-bongo-264115.internal
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://instance-1.southamerica-east1-a.c.sincere-bongo-264115.internal:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_CREATE_TOPICS: "movieRatings:1:1"
    depends_on:
        - zookeeper


  airflow-webserver:
    container_name: "airflow-webserver"
    image: puckel/docker-airflow
    # restart: always
    depends_on:
        - postgres
    environment:
        - LOAD_EX=n
        - EXECUTOR=Local
        # - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/key_file.json&extra__google_cloud_platform__scope=https://www.googleapis.com/auth/cloud-platform&extra__google_cloud_platform__project=sincere-bongo-264115&extra__google_cloud_platform__num_retries=5
        # - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/key_file.json&extra__google_cloud_platform__scope=https://www.googleapis.com/auth/cloud-platform&extra__google_cloud_platform__project=sincere-bongo-264115


        - FERNET_KEY='81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs='
        - AIRFLOW__CORE__FERNET_KEY='81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs='
        # docker run puckel/docker-airflow python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)"

    logging:
        options:
            max-size: 10m
            max-file: "3"
    volumes:
        - ./dags:/usr/local/airflow/dags
        - ./requirements.txt:/requirements.txt
        - ./key_file.json:/key_file.json
        # - ./plugins:/usr/local/airflow/plugins
    ports:
        - "8080:8080"
    command: webserver

  postgres:
    container_name: "postgres"
    image: postgres:9.6
    environment:
        - POSTGRES_USER=airflow
        - POSTGRES_PASSWORD=airflow
        - POSTGRES_DB=airflow
    logging:
        options:
            max-size: 10m
            max-file: "3"

# docker build . --tag=ratings-generator:latest
  ratings-generator:
    container_name: "ratings-generator"
    # image: ratings-generator:latest
    build: ./generate_ratings/
    depends_on: 
      - kafka


  mysql-db:
    container_name: "mysql-db"
    image: mysql:5.7
    restart: always
    environment:
      MYSQL_DATABASE: 'test'
      # So you don't have to use root, but you can if you like
      MYSQL_USER: 'user'
      # You can use whatever password you like
      MYSQL_PASSWORD: '12345'
      # Password for root access
      MYSQL_ROOT_PASSWORD: '12345'
    ports:
      # <Port exposed> : < MySQL Port running inside container>
      - '3307:3306'
    expose:
      # Opens port 3306 on the container
      - '3306'
      # Where our data will be persisted
    volumes:
      - mysql-db:/var/lib/mysql
  
  # Names our volume
volumes:
  mysql-db: